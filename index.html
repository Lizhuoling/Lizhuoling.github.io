
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

  <title>Zhuoling Li's Homepage</title>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Zhuoling Li is currently pursuing the PhD degree in the University of Hong Kong.">
  <meta name="keywords" content="Zhuoling Li, 李卓凌, lizhuoling, Zhuoling, Deep Learning, Tsinghua University, THU, Computer, Vision">
  <meta name="author" content="Zhuoling Li" />

  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
  </style>

  <link rel="icon" type="image/png" href="images/icon.png">
 
</head>


<body class="w3-content" style="max-width:1000px">

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-bar-block w3-black w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b>Zhuoling</b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#home" class="w3-bar-item w3-button">Home</a>
    <a href="#news" class="w3-bar-item w3-button">News</a>
    <a href="#experience" class="w3-bar-item w3-button">Experience</a>
    <a href="#publication" class="w3-bar-item w3-button">Publication</a>
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-bar w3-top w3-hide-large w3-black w3-xlarge">
  <div class="w3-bar-item w3-padding-24">Zhuoling</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:150px">

  <!-- Push down content on small screens -->
  <div class="w3-hide-large" style="margin-top:83px"></div>

<!-- The Home Section -->
    <div class="w3-container w3-center w3-padding-32" id="home">
      <img style="width: 80%;max-width: 320px" alt="profile photo" src="images/LiZhuoling_Circle.png">
      <h1>Zhuoling Li</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
          I am now pursuing the PhD degree at <a href="https://www.cs.hku.hk/">the Computer Science Department</a>, <a href="https://www.hku.hk/">the University of Hong Kong</a>, where I work on embodied intelligence and 3D visual perception, etc. Before that, I received the Bachelor degree from <a href="http://aia.hust.edu.cn/">School of Artificial Intelligence and Automation</a>, <a href="https://www.hust.edu.cn/">Huazhong University of Science and Technology</a>, and received the Master Degree from <a href="https://www.sigs.tsinghua.edu.cn/">Shenzhen International Graduate School</a>, <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>. I have published many papers in top conferences and journals, and also served as reviewers for several top conferences and IEEE Transactions journals, e.g., CVPR, ICCV, ECCV.
        </p>
        <p class="w3-center">
          <a href="mailto:lizhuoling@connect.hku.hk">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=2r6ejykAAAAJ">Google Scholar</a>
        </p>
        </tbody></table>
  </div>

<!-- The News Section -->
  <div class="w3-container w3-light-grey w3-padding-32" id="news">
   <h2>News</h2>
	<p><li> 3/2024, One of my papers is accepted by <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR2024</a>.</li></p>
	<p><li> 6/2023, We achieve 1st place in the <a href="https://opendrivelab.com/AD23Challenge.html#overview">OpenLane Topology Challenge</a>, which is held in conjunction with the CVPR 2023 Workshop on Endto-End Autonomous Driving and CVPR 2023 Workshop on
Vision-Centric Autonomous Driving Workshop.</li></p>
	<p><li> 11/2022, One of my papers is accepted by <a href="https://aaai.org/Conferences/AAAI-23/">AAAI2023</a>.</li></p>
	<p><li> 04/2022, One of my papers is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688">IEEE Transactions on Artificial Intelligence</a>.</li></p>
	<p><li> 03/2022, One of my papers is selected as an oral presentation of <a href="https://cvpr2022.thecvf.com/">CVPR2022</a>.</li></p>
  	<p><li> 03/2022, Two of my papers are accepted by <a href="https://cvpr2022.thecvf.com/">CVPR2022</a>.</li></p>
  </div>

<!-- The Experience Section -->
  <div class="w3-container w3-padding-32" id="experience">
   <h2>Experience</h2>
  	<p><li> 09/2016~06/2020, I was pursuing the Bachelor degree in <a href="https://www.hust.edu.cn/">Huazhong University of Science and Technology</a>. During this period, I was advised academically by Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=xm_RRHEAAAAJ">Shiping Wen</a> and Prof. <a href="http://faculty.hust.edu.cn/liyuanzheng2/zh_CN/index.htm">Yuanzheng Li</a>. </li></p>
	<p><li> 03/2020~06/2020, I was a full-time intern at the Knowledge Computing Group, <a href="https://www.msra.cn/">Microsoft Research Asia</a>. </li></p>
  	<p><li> 09/2020~06/2023, I was pursuing the Master degree in <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>. During this period, I was advised academically by Prof. <a href="https://scholar.google.com/citations?user=eldgnIYAAAAJ">Haoqian Wang</a>. </li></p>
	<p><li> 05/2021~11/2021, I was a full-time intern at the Autonomous Driving Group, <a href="https://www.noahlab.com.hk/">Huawei Noah's Ark Lab</a>. </li></p>
	<p><li> 12/2022~07/2023, I was a full-time intern at the Base Model Group, <a href="https://www.megvii.com/">MEGVII Technology</a>. </li></p>
	<p><li> 09/2023~Now, I was pursuing the PhD degree in <a href="https://www.hku.hk/">the University of Hong Kong</a>. During this period, I was advised academically by Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=4uE10I0AAAAJ">Hengshuang Zhao</a>. </li></p>
  </div>
  
 <!-- The Publications Section -->
  <div class="w3-container w3-padding-32"" id="publication">
    <h2>Publication</h2>
      <p class="w3-left-align" style="line-height:200%">
        I am interested in developing outstanding embodied artificial intelligence systems and vision perception models. 
      </p>
						       
    <h4> Preprint Papers:</h4>
						       
    <ol>
    <p>
      <li><strong>The 1st-place Solution for CVPR 2023 OpenLane Topology in Autonomous Driving Challenge</strong>
      <br>
      Dongming Wu, Fan Jia, Jiahao Chang, <strong>Zhuoling Li</strong>, Jianjian Sun, Chunrui Han, Yingfei Liu, Zheng Ge, Tiancai Wang.
      <br>
      2023 | Arxiv | <a style="color: #447ec9" href="https://arxiv.org/pdf/2306.09590.pdf">paper</a>
    </p>

    <p>
      <li><strong>GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping</strong>
      <br>
      <strong>Zhuoling Li</strong>, Chunrui Han, Zheng Ge, Jinrong Yang, En Yu, Haoqian Wang, Hengshuang Zhao, Xiangyu Zhang. (First author)
      <br>
      2023 | Arxiv | <a style="color: #447ec9" href="https://arxiv.org/pdf/2307.09472.pdf">paper</a>
    </p>

    <p>
      <li><strong>VoxelFormer: Bird's-Eye-View Feature Generation based on Dual-view Attention for Multi-view 3D Object Detection</strong>
      <br>
      <strong>Zhuoling Li</strong>, Chuanrui Zhang, Wei-Chiu Ma, Yipin Zhou, Linyan Huang, Haoqian Wang, SerNam Lim, Hengshuang Zhao (First author)
      <br>
      2023 | Arxiv | <a style="color: #447ec9" href="https://arxiv.org/pdf/2304.01054.pdf">paper</a>
    </p>

    <p>
      <li><strong>Delving into the Pre-training Paradigm of Monocular 3D Object Detection</strong>
      <br>
      <strong>Zhuoling Li</strong>, Chuanrui Zhang, En Yu, Haoqian Wang. (First author)
      <br>
      2022 | Arxiv | <a style="color: #447ec9" href="https://arxiv.org/pdf/2206.03657.pdf">paper</a>
    </p>
    </ol>
						       
						       
    <h4> Conference Papers:</h4>

    <ol>

      <p>
      <li><strong>UniMODE: Unified Monocular 3D Object Detection</strong>
      <br>
      <strong>Zhuoling Li</strong>, Xiaogang Xu, SerNam Lim, Hengshuang Zhao. 
      <br>
      2022 | <em>AAAI</em> 2022 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2212.01568.pdf">paper</a>
      </p>
											  
      <p>
      <li><strong>Generalizing Multiple Object Tracking to Unseen Domains by Introducing Natural Language Representation</strong>
      <br>
      En Yu, Songtao Liu, <strong>Zhuoling Li</strong>, Jinrong Yang, Zeming Li, Shoudong Han, Wenbing Tao. 
      <br>
      2022 | <em>AAAI</em> 2022 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2212.01568.pdf">paper</a>
      </p>										  
     
      <p>
      <li><strong>Diversity Matters: Fully Exploiting Depth Clues for Reliable Monocular 3D Object Detection</strong>
      <br>
      <strong>Zhuoling Li<sup>*</sup></strong>, Zhan Qu<sup>*</sup>, Yang Zhou, Jianzhuang Liu, Haoqian Wang, Lihui Jiang. (First author)
      <br>
      2022 | <em>CVPR</em> 2022 | <a style="color: #447ec9" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Diversity_Matters_Fully_Exploiting_Depth_Clues_for_Reliable_Monocular_3D_CVPR_2022_paper.pdf">paper</a> | Oral
      </p>
										       
      <p>
      <li><strong>Towards Discriminative Representation: Multi-view Trajectory Contrastive Learning for Online Multi-object Tracking</strong>
      <br>
      En Yu<sup>*</sup></strong>, <strong>Zhuoling Li<sup>*</sup></strong></strong>, Shoudong Han. (Co-First author)
      <br>
      2022 | <em>CVPR</em> 2022 | <a style="color: #447ec9" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Towards_Discriminative_Representation_Multi-View_Trajectory_Contrastive_Learning_for_Online_Multi-Object_CVPR_2022_paper.pdf">paper</a>
      </p>
      
     </ol>
     
    <h4> Journal Papers:</h4>
    <ol>
											     
      <p>
      <li><strong>Efficient Few-shot Classification via Contrastive Pre-training on Web Data</strong>
      <br>
      <strong>Zhuoling Li</strong>, Haohan Wang, Tymoteusz Swistek, En Yu, Haoqian Wang. (First author)
      <br>
      2022 | <em>IEEE Transactions on Artificial Intelligence</em> | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9763033">paper</a>
      </p>	
											     
      <p>
      <li><strong>Relationtrack: Relation-aware Multiple Object Tracking with Decoupled Representation</strong>
      <br>
      En Yu<sup>*</sup>, <strong>Zhuoling Li<sup>*</sup></strong></strong>, Shoudong Han, Hongwei Wang. (Co-First author)
      <br>
      2022 | <em>IEEE Transactions on Multimedia</em> | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9709649">paper</a>
      </p>	
      
      <p>
      <li><strong>Few-shot Steel Surface Defect Detection</strong>
      <br>
      Haohan Wang<sup>*</sup>, <strong>Zhuoling Li<sup>*</sup></strong></strong>, Haoqian Wang. (Co-First author)
      <br>
      2021 | <em>IEEE Transactions on Instrumentation and Measurement</em> | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9623623">paper</a>
      </p>
      
      <p>
      <li><strong>Deep Learning based Densely Connected Network for Load Forecasting</strong>
      <br>
      <strong>Zhuoling Li</strong>, Yuanzheng Li, Yun Liu, Ping Wang, Renzhi Liu, Hoay Beng Gooi. (First author)
      <br>
      2020 | <em>IEEE Transactions on Power Systems</em> | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9311801">paper</a>
      </p>
      
      <p>
      <li><strong>CLU-CNNs: Object detection for medical images</strong>
      <br>
      <strong>Zhuoling Li</strong>, Minghui Dong, Shiping Wen, Xiang Hu, Pan Zhou, Zhigang Zeng. (First author)
      <br>
      2019 | <em>Neurocomputing</em> | <a style="color: #447ec9" href="https://pdf.sciencedirectassets.com/271597/1-s2.0-S0925231219X0020X/1-s2.0-S0925231219305521/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEBwaCXVzLWVhc3QtMSJIMEYCIQCb5EHPoJh6yRDrVevVOlzvAvP%2BWLXZY%2FVaMWfhHCRk1QIhAOjJo9SiuHRcPv%2FA%2FHtQeXu359J9Mrx3Zna%2BZlXFlOi0KoMECKX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBBoMMDU5MDAzNTQ2ODY1Igx62wlURD3RnK1NFQYq1wPxxCA8sQOAjIvW0VbhAZSnrP9fcgz3Nj%2F2Lwu%2F4xeFX%2FbCxVckAellY9UGaz%2FmTnbTFEzq5ALRJtzHF4sPDtcdsDzuFY6xsB0p7%2FkzDsIbPvWTrZdHTUwAX0pbsFrjqZM4GKHijcbxiMxqTZZVYo5rH2jtin9KNemuatfmMZno3wRxn62vZPXOMNl4YHGbbOqhjtG3fY5FydImr1dZIvM0oH6eeec8GpTpidnextcIpwkDCotzRmpVYR8xrYyCtPPBH4Dx8nofH4jUb3V2D8PpjDZb0zPLKzFlWTA5AebzaTO8a7RSEFbgH0lh%2B2GYPRT3H7PlwGBcLtSvfOgpqDdEBoD8orioSH1glSyfANY7b%2ByCwzS346ZESNBZsrEXGz9NpiIsr8ZQGiI90utkYy9y9UGjPhdTIded8My1P8D31bGkON3%2FWdStOuURG74tB6ceFHjGlRaS2K7ZbtU378e8O9%2Bdz89P4LEvjwRz%2FM04Ib%2FTdOX9PYznRSG%2BUDSOznwHzZ8We6dzpfKsjmKfJHsE0%2F4lzK1FcZpHD3iLRJzRChJYGxYfHkNUaltwGfc45tp5lf7lSLamfbWNzaZBJxIIxo7jCFj5lVrh0At403Q6kuvtIz%2BFSSwwo6OBkgY6pAHtZeU1BYsUG4JxdybcUIW1uvzgw0FzMrru58XZqB7aD%2BH1HsvAMMpV4Tzzcd%2FS6zLgmHwRky8HQwsHujfRsonqMNqBCp2fxqa%2Bo6qOJZRAfwobex3tV6cbc6rmmBH6irCh6H%2FIYbatR3MXgVZ3IPJsBnigdrPB5ofnDWCzp6NZqUMFRtYMmQLST0nd%2FfZZb6hY4LXTmRd7QvPe8PkdsRVLuPy5QA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220327T122642Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYVVCUP3XI%2F20220327%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=e4ebeddb7db9b0107929d4203beafc382543e1b104f0d231a21ce6f579bf648b&hash=e3ef5969ad5f9e7d252e66daf1fa80fe89af5cf6b0350167e5352858a591c85b&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0925231219305521&tid=spdf-804b6d5f-a377-43f8-9275-aea20a253c00&sid=b738fb7a77151449b48b8d9322b87f273d70gxrqa&type=client&ua=58045c5b5703010357&rr=6f281dae6f13984f">paper</a> 
      | Highly Cited Paper
      </p>
											     
    </ol>

    </p>
  </div>


  <div class="w3-light-grey w3-center w3-padding-24">

  <!-- Default Statcounter code for Yunhe Wang's Homepage
  https://www.wangyunhe.site -->
  No.
  <script type="text/javascript">
  var sc_project=12347113; 
  var sc_invisible=0; 
  var sc_security="21aca5d1"; 
  var sc_https=1; 
  var scJsHost = "https://";
  document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
  "statcounter.com/counter/counter.js'></"+"script>");
  </script> Visitor Since Feb 2022. Powered by <a href="https://www.w3schools.com/w3css/default.asp" title="W3.CSS" target="_blank" class="w3-hover-opacity">w3.css</a>
  <noscript>
    <div class="statcounter"><a title="Web Analytics Made Easy -
  StatCounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12347113/0/21aca5d1/0/"
  alt="Web Analytics Made Easy - StatCounter"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

  </div>

  <!-- End page content -->
</div>

<script>
// Accordion 
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>

</body>
</html>
